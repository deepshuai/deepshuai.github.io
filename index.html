<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="努力，任何时候都不会晚。">
<meta property="og:url" content="https://deepshuai.github.io/index.html">
<meta property="og:site_name" content="努力，任何时候都不会晚。">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="努力，任何时候都不会晚。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://deepshuai.github.io/">





  <title>努力，任何时候都不会晚。</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-89140122-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">努力，任何时候都不会晚。</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deepshuai.github.io/2019/04/02/论文-Cambricon-X-An-Accelerator-for-Sparse-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Deepshuai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/deepshuai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="努力，任何时候都不会晚。">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/02/论文-Cambricon-X-An-Accelerator-for-Sparse-Neural-Networks/" itemprop="url">论文-Cambricon-X: An Accelerator for Sparse Neural Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-02T16:06:55+08:00">
                2019-04-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deepshuai.github.io/2019/04/02/论文-Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Deepshuai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/deepshuai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="努力，任何时候都不会晚。">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/02/论文-Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks/" itemprop="url">论文-Cambricon: An Instruction Set Architecture for Neural Networks</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-02T16:06:26+08:00">
                2019-04-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/硬件设计/" itemprop="url" rel="index">
                    <span itemprop="name">硬件设计</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Cambricon又名DianNaoYu，这篇论文研究和发表的时候，与法国Temam教授的合作已经结束。如果说DianNao开创了人工智能加速器设计的研究，后续的DaDianNao、ShiDianNao、PuDianNao分别各有侧重，在不同的应用场景下对深度神经网络进行加速器的设计，那么Cambricon追求的是在更为泛化的层面上对深度神经网络加速器进行设计，是深度学习加速器真正的能大规模的应用到具体的场景之中的重要工作。<br>之后研究团队注册了公司，名字就是Cambricon，专注端+云的智能处理器生态的培养，而最核心的东西就是指令集，接下来具体了解一下在Cambricon指令集之上的处理器的设计细节。</p>
<h2 id="ISA介绍"><a href="#ISA介绍" class="headerlink" title="ISA介绍"></a>ISA介绍</h2><h3 id="整体介绍"><a href="#整体介绍" class="headerlink" title="整体介绍"></a>整体介绍</h3><p>Combricon为64-bit的定长指令集架构，指令集整体分为四个大类，分别是控制类指令/数据存取类指令/算术计算类指令/逻辑运算类指令。大致的分类如下表所示：</p>
<p><img src="/2019/04/02/论文-Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks/ISAOverview.png" width="700" align="center"></p>
<p><strong>控制指令：</strong> 分为两种，jump和conditional branch。</p>
<p><img src="/2019/04/02/论文-Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks/ControlIS.png" width="400" align="center"></p>
<p><strong>数据存取指令：</strong> 支持标量、向量和矩阵三种数据形式，三种数据形式分别支持load/store/move三种操作。其中load是将一定规模的数据从main memory加载到on-chip scratchpad memory，store的数据流动方向刚好相反，而move指令是数据在calar GPRs和on-chip scratchpad memory之间的存取。下图是vector load指令：</p>
<p><img src="/2019/04/02/论文-Cambricon-An-Instruction-Set-Architecture-for-Neural-Networks/VLoad.png" width="400" align="center"></p>
<p><strong>算术计算指令/逻辑计算指令</strong> 作为设计的重点后面章节具体介绍。</p>
<h2 id="计算-逻辑指令"><a href="#计算-逻辑指令" class="headerlink" title="计算/逻辑指令"></a>计算/逻辑指令</h2><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deepshuai.github.io/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Deepshuai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/deepshuai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="努力，任何时候都不会晚。">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/" itemprop="url">论文-PuDianNao: A Polyvalent Machine Learning Accelerator</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-29T03:05:41+08:00">
                2019-03-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/硬件设计/" itemprop="url" rel="index">
                    <span itemprop="name">硬件设计</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>PuDianNao于之前DianNao系列的论文主要专注于神经网络不同，论文想通过总结目前主流的机器学习算法的计算操作特点以及访存模式，设计出更加范性的机器学习加速器。</p>
<h2 id="典型机器学习算法分析"><a href="#典型机器学习算法分析" class="headerlink" title="典型机器学习算法分析"></a>典型机器学习算法分析</h2><font color="red">论文中的体系结构支持k-近邻算法、k-means算法、深度神经网络算法、线性回归算法、支持向量机算法、朴素贝叶斯算法、分类树算法共7种机器学习算法。</font>

<h2 id="加速器整体架构设计"><a href="#加速器整体架构设计" class="headerlink" title="加速器整体架构设计"></a>加速器整体架构设计</h2><p>PuDianNao的整体硬件架构如下：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/pudiannao.png" width="500" align="center"><br>PuDianNao主要是由若干个功能单元(FUs)，每个FU均由一个MLU和一个ALU组成、三个数据缓存(Hot Buffer、Cold Buffer、Output Buffer)、一个指令缓存(Instruction Buffer)、一个控制模块以及DMA控制器等组成。</p>
<h3 id="功能单元-Function-Unit"><a href="#功能单元-Function-Unit" class="headerlink" title="功能单元 Function Unit"></a>功能单元 Function Unit</h3><p>Function Unit是PuDianDao的执行单元，每个FU均由一个MLU和一个ALU组成，MLU是用于机器学习算法的硬件定制，而ALU用于常规的计算控制任务。由此可见MLU的设计是机器学习算法加速的核心。针对机器学习的算法加速，MLU采用Counter、Adder、Multiplier、Adder tree、acc和Misc六级流水线，具体的流水线结构如下图所示：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/MLU.png" width="500" align="center"><br>关于流水线的设计，并不是所有的算法都会全部用到流水线的每一级，但流水线将算法所设计的主要运算包含在内。尤其是Multiplier 和 Adder Tree的流水线的组合对dot product提供了支持，这种向量/矩阵dot product是LR/SVM/DNN算法中的高频操作；对于数据的宽度大于硬件逻辑宽度的时候，通过ACC对partial sum进行缓存；而Misc流水段通过了非线性激活函数(sigmod/tanh/ReLU)的线性插值近似操作和top-k/tail-k的功能。</p>
<p>相对于MLU流水线的设计，ALU的设计相对简单，主要实现MLU中未支持的逻辑运算操作，主要是机器学习算法中的非高频操作。这里的设计思路可能是说：<strong>作为机器学习加速器，通常的做法是将机器学习的高频计算放在加速器上完成，而非高频的逻辑操作和控制逻辑放在宿主CPU上完成，这里添加ALU主要是为了让加速器尽可能独立的处理机器学习算法，减少宿主CPU和加速器MLU之间的协同交互开销。</strong></p>
<h3 id="数据位宽"><a href="#数据位宽" class="headerlink" title="数据位宽"></a>数据位宽</h3><p>为了减少芯片的面积、降低功耗、增加数据吞吐量、提高计算速度、减少通信带宽，PuDianNao采用mixed precision，在流水线的Adder/Multiplier/Adder tree这三个stage支持16位的浮点运算，而对Counter/Acc/Misc这三个stage仍然使用32位的浮点数。主要的考虑因素是Counter/Acc/Misc分别靠输入数据和最终的结果数据很近，容易出现溢出现象，而Adder/Multiplier/Adder tree最为中间结果，溢出的风险较小，这也是一种设计在精度和加速性能之间的权衡。<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/precision.png" width="400" align="center"><br>由上表可以看出，采用fp16和fp32的混合精度计算的算法精度和全精度相比没有特别大的精度损失。<strong><font color="red">这里推荐一篇NVIDIA的混合精度训练神经网络以及指导硬件设计的论文：<a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">Mixed Precision Training</a>，该方法训练过程中通过一些额外的trick可以进一步避免因fp16低精度带来的算法性能损失，这里不展开介绍。</font></strong></p>
<h3 id="缓存设计"><a href="#缓存设计" class="headerlink" title="缓存设计"></a>缓存设计</h3><p>PuDianNao中包含是三个数据缓存(HotBuf、ColdBuf、OutputBuf)和一个指令缓存(InstBuf)。除了将指令缓存和数据缓存分开，同时将数据缓存进行进一步的划分，主要考虑的是一方面机器学习算法多是SIMD的计算形式，对数据的带宽要求远远大于指令缓存的带宽；另一方面，数据带宽进行更加细粒度的划分，是因为计算任务中数据复用程度不同，需要的带宽不相同。同时，为了进一步的降低功耗和芯片的面积，HotBuf和ColdBuf由于只进行读操作，因此采用但端口的SRAM（只读），而OutputBuf要支持读写操作，采用双端口的SRAM（可读写）。</p>
<h2 id="控制和代码生成"><a href="#控制和代码生成" class="headerlink" title="控制和代码生成"></a>控制和代码生成</h2><p>PuDiannao的Control Module起到指令译码和任务分发的功能。PuDianNao通过Control Module来对计算任务进行描述，将指令进行译码并将需要执行的操作发送给FU。<strong><font color="red">这里需要注意的是，到PuDianNao论文的时候，寒武纪团队还没有针对加速器设计提出一整套的指令集的设计，所以要对每种算发的计算过程进行指令抽象，指令级别的编程要对硬件的架构非常的了解，这也是后面<a href="https://dl.acm.org/citation.cfm?id=3001179" target="_blank" rel="noopener">Cambricon的指令集</a>论文出现让整个硬件设计大规模的工业应用成为可能。</font></strong></p>
<p>下表是PuDianNao中涉及到的指令：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/InstName.png" width="500" align="center"><br>而基于以上指令的K-Means算法的实现如下表所示：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/InstKmeans.png" width="500" align="center"></p>
<h2 id="实验及结果分析"><a href="#实验及结果分析" class="headerlink" title="实验及结果分析"></a>实验及结果分析</h2><p>实验主要的评估集中在加速比和功耗两个方面，同时最终给出了各个模块的芯片面积和占比。实验分别基于C和verilog仿真器实验环境，Verilog(65nm工艺)精度更高，速度慢，C仿真速度快但会有精度的损失。</p>
<p>下图是实验的benchmarks和Dataset：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/BenchmarkDataset.png" width="500" align="center"><br>评估的baseline是GPU(NVIDIA K20M, 3.2TFlops peak，5GB显存，208GB/s显存带宽，28nm工艺， CUDA SDK5.5）。实验将每种算法的training和predicition分开对比：</p>
<h3 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h3><p>在比较之前，先进行GPU和CPU的加速比比较：其中CPU with 256-bit SIMD (Intel Xeon E5-4620 Sandy Bridge-EP, 2.2GHz, 1TB memory, gcc compilation flag “-O3 -ftree-vectorize -march=native”)<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/GPUCPU.png" width="500" align="center"><br>通过上图可以看出GPU相对与CPU有数量级级别的加速。</p>
<p>PuDianNao相对于GPU的加速比：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/PuDianNaoGPU.png" width="500" align="center"><br>PuDianNao在多数算法上相对于GPU有更为突出的性能，但在部分算法phase中表现并没有比GPU好，其主要原因应该是这些算法中存在着大量的矩阵乘的运算，GPU中存在大量的寄存器，这样就可以临时的存储partical number，而PuDianNao并没有配置大规模的寄存器堆，没有办法对中间结果的保存，这样就需要FU和dataBuffer之间进行频繁的通信和交互，这样就限制了性能的提升。</p>
<h3 id="功耗比较"><a href="#功耗比较" class="headerlink" title="功耗比较"></a>功耗比较</h3><p>GPU/PuDianNao相对于CPU都有显著的性能提升，但PuDianNao由于硬件资源的限制，相对于GPU的性能提升非常的有限，但是就硬件的功耗而言，相对于GPU由非常明显的优势：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/GPUPuDianNao.png" width="500" align="center"><br>由上图可以看出，PuDianNao相对于GPU的功耗由两个数量级的提升。</p>
<h3 id="芯片面积"><a href="#芯片面积" class="headerlink" title="芯片面积"></a>芯片面积</h3><p>下图是PuDianNao芯片的整体layout信息：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/layout.png" width="400" align="center"><br>而具体的每个模块面积和占别以及各个模块的功耗情况如下图所示：<br><img src="/2019/03/29/论文-PuDianNao-A-Polyvalent-Machine-Learning-Accelerator/Infor.png" width="500" align="center"><br>FU(Functional Unit)和CB（Cold Buffer）占了较大的面积，CM（Control Module）的面积不大，但是功耗并不小。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deepshuai.github.io/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Deepshuai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/deepshuai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="努力，任何时候都不会晚。">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/" itemprop="url">论文-ShiDianNao: Shifting Vision Processing Closer to the Sensor</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-28T14:33:05+08:00">
                2019-03-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/硬件设计/" itemprop="url" rel="index">
                    <span itemprop="name">硬件设计</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;ShiDianNao这篇论文更像是针对具体应用场景的体系结构&amp;优化的工作，但是对加速的的设计细节介绍的最为详细，无论是访存体系还是计算单元的设计都介绍的非常的细致。<strong>文章的主要思想：利用卷集神经网络(CNNs)权值共享的特性，将CNNs模型参数整体加载到SRAM构成的高速存储里，减少DRAM访问带来的开销;同时将加速器与视频图像传感器相连，减少额外的访存操作(传统模式：先将传感器采集的数据放入DRAM，然后通知加速器进行处理，在处理的过程中还要再一次的访问DRAM获取传感器数据)。</strong></p>
<p><strong><font color="red">按照上面的思路，硬件的设计不可避免的会碰到以下问题：</font></strong><br><strong>1、SRAM被分配多大的容量？</strong> 既然要将CNNs模型参数整体加载到SRAM上，那么就会面对SRAM容量的取舍，容量大了功耗增加，芯片面积增加；容量过小，模型不能完全hold住，对计算性能带来影响。<br><strong>2、计算单元和CNNs如何映射？</strong> 最简单的方式就是每个神经元映射一个硬件的计算单元，但是CNNs模型的规模各不相同并且差别很大，在一定程度上限制了灵活性；另一种思路和之前的论文一致，采用基于时分复用的方式，执行计算和加载数据流水线的方式进行，而计算过程中是一次计算一个特定的feature map的output还是一次计算多个仍然是需要衡量的问题。<br><strong>3、Data locality是否眼进行深度的挖掘和优化？</strong> 数据局部性的优化处理也可能在一定程度上提高性能，如CNNs卷积层的计算过程中一组input neuron可能会被多个output neuron使用，所以在计算单元中引入一些本地存储可能会节省访存的开销，但同时会增加硬件设计的复杂度。<br>ShiDianNao论文最大的贡献就是对以上问题都进行了分析和处理，给出了非常精巧的设计方案。</p>
<h2 id="整体硬件架构"><a href="#整体硬件架构" class="headerlink" title="整体硬件架构"></a>整体硬件架构</h2><p>首先看一下ShiDianNao的整体硬件结构：<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/shidiannao架构.png" width="400" align="center"></p>
<h3 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h3><p>左侧NBin/NBout/SB是SRAM快用于数据的存储，而具体每一部分的存储空间如下表所示：<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/存储容量.jpg" width="250" align="center"></p>
<font color="red">这里最要解决的是问题1，也就是SRAM分配容量的问题，SB的size为128K，对于一般的CNNs网络是远远不够的，可能结合模型压缩算法可以解决该容量的问题。</font>

<p>DianNao系列的论文均采用类似的NBin/NBout/SB分离的方式，根据CNNs计算任务的特点，NBin存储input neuron，NBout存储output Neuron，SB存储模型参数。这三类存储，要求SB能够hold住模型全部的参数，NBin/NBout能够hold住完整的inpout/output neuron。因为模型的参数会被反复的使用，存在SRAM中可以减少从DRAM中加载数据时的时间开销，而输入数据中的raw data(input image)处理一次之后不再使用，所以只需要每个网络层在计算的时候input/output neuron在SRAM中即可。</p>
<h3 id="NFU"><a href="#NFU" class="headerlink" title="NFU"></a>NFU</h3><p>芯片的右侧是NFU(Neural Functional Unit)，是由若干个PE组成的$x*y$大小的计算阵列，每个PE中有一个乘法器和一个加法器、若干个寄存器、两组用于在PE阵列水平/竖直方向进行数据交互的FIFO医护辅助的控制逻辑，具体的结构如下图：<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/PE.png" width="400" align="center"><br>NFU的计算结果通过一个ALU写入到NBout中，ALU中实现并行度并不是很高的运算，如平均池化的除操作、非线性激活计算等，其中非线性激活运算采用分段函数进行插值近似，在精度损失很小的情况下获得更高的性能和功耗收益。</p>
<p>NFU的PE阵列中对PE之间的data propagation进行了支持，引入改设计的目的是为了减少PE和SRAM之间的通信。对于卷积运算，同一个feature map的不同output neuron，stride没有超出kernel size的情况下，输入数据是由于部分是重复的，而PE间的data propagation引入就是为了将不同output neuron之间重复的那部分input neuron利用起来，从而减少SRAM访问的频率，可以很大程度上节省带宽，降低功耗，提升性能。其具体的收益如下图所示：<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/pedata.png" width="400" align="center"><br>从卷积核尺寸的角度，卷积核的尺寸越大，收益更加的明显；上图是以32x32 input feature map + 5x5卷积核为例，随着PE数目的增加，在没有Inter-PE data propagation支持的情况下，带宽也会随之增加，但是在支持Inter-PE data propagation的情况下，带宽需求的增长十分的有限。</p>
<h3 id="Buffer-Controller"><a href="#Buffer-Controller" class="headerlink" title="Buffer Controller"></a>Buffer Controller</h3><p>Buffer Controller是ShiDianNao引入的一个重要的结构，用于片上存储NBin/NBout/SB与计算部件NFU之间的数据交互的co-ordinator。Buffer Controller负责两个功能：<br><strong>1、以流式的方式，逐层的为NFU提供计算所需要的数据；</strong><br><strong>2、缓冲NFU计算的partial的结果，汇总完一整个layer/feature map上所有的output neuron之后写会NBout。</strong><br>同时为了高效的支持CNN模型中不同操作对应的访存特点，在Buffer controller中提供多种read mode的支持：<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/readmode.png" width="400" align="center"><br>(a)读取多个banks，#0-&gt;#py-1<br>(b)读取多个banks，#py-&gt;#2py-1<br>(c)读取一个bank<br>(d)读取单个神经元<br>(e)通过给定的step size来读取神经元<br>(f)每个bank中读取一个神经元<br>对于卷积层，使用(a)/(b)从NBin中获取px*py，而(e)主要处理应于卷积核step size &gt; 1的情形；对于池化层，使用mode(a)(b)(c)(e)和(f)，对于normalization layer依然使用mode(a)(b)(c)(e)和(f)，而分类层使用mode(d)从output neuron中加载input neuron。</p>
<h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><h3 id="对比性能"><a href="#对比性能" class="headerlink" title="对比性能"></a>对比性能</h3><p>评估环境的搭建使用的是Synopsys提供的EDA工具，65nm；实验的Baseline则选取了CPU（Intel Xeon E7-8830/2.13GHZ/1TB Memory）、GPU（NVIDIA K20M/5GB显存/3.52TFlops/28nm/Caffe）以及DianNiao项目的第一个工作成果DianNao，由于SRAM存储容量的限制，不能使用大的神经网络模型，所有的网络模型都是比较小的规模，在这10个Benchmark model里，layer wise的神经元最多消耗45KB SRAM存储，而模型的权重最多只消耗118KB SRAM，足以被ShiDianNao目前配备的288KB SRAM所支持），以下是所有实验的模型：<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/result.png" width="500" align="center"></p>
<p>评估所用的ShiDianNao，其NFU是由8*8=64个PE组成，在一个指令周期内支持64组乘加组合运算，存储上由64KB的NBin，64KB的NBout和128KB的SB以及32KB的指令Buffer组成且均采用SRAM的存储介质，具体的实验结果如下图所示：<br>相对于CPU、GPU、DianNao系列在速度方面的实验<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/sp.png" width="500" align="center"><br>相对于CPU、GPU、DianNao在能耗方面的实验：<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/en.png" width="500" align="center"><br>以上的图可以看到，相较于CPU和GPU的bseline，其速度有显著的提升(46.8x than CPU，18.9x than GPU)，相较于DianNao依然有1.87X的速度提升，其功耗方面ShiDianNao依然有明显的优势。</p>
<h3 id="功耗分配和面积分配"><a href="#功耗分配和面积分配" class="headerlink" title="功耗分配和面积分配"></a>功耗分配和面积分配</h3><p>下表是ShiDianNao的功耗和面积分配情况：<br><img src="/2019/03/28/论文-ShiDianNao-Shifting-Vision-Processing-Closer-to-the-Sensor/分配.png" width="500" align="center"><br>由于ShiDianNao的一个策略就是将模型参数通过SRAM本地化，所有就芯片的面积而言，SB/NBin/NBout占用了绝大本分；而各个部件的功耗情况则明显不同，NFU占用了所有能耗的84%左右，着主要是因为这将NFU的计算功耗计算在内的同时，也将Inter-PE data propagation这部分本来需要访存的操作转移到NFU上。</p>
<p><strong><font color="red">思考：</font></strong></p>
<font color="red">其实就ShiDianNao的硬件设计而言，其SRAM本地化模型参数的思想和仅仅288KB的SRAM是比较矛盾的，因为现有的高精度模型往往都是数百MB的规模，芯片的设计很难满足通用型的要求。解决的方式有两个：(1)继续硬件的优化，在面积/功耗/成本各方面作出比较好的权衡；(2)模型压缩技术或许是一种不错的决绝办法，软件和算法相对于硬件来说有更大的灵活性，硬件在设计的时候充分考虑算法的计算特性，同时算法的部署过程中也要实时调整，在硬件上有更好的执行效率。</font>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deepshuai.github.io/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Deepshuai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/deepshuai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="努力，任何时候都不会晚。">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/" itemprop="url">论文-DaDianNao: A Machine-Learning Supercomputer</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-25T21:57:23+08:00">
                2019-03-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/硬件设计/" itemprop="url" rel="index">
                    <span itemprop="name">硬件设计</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;之前的DianNao论文可以看作是硬件设计的基础，在此之上，对深度学习芯片所面对的应用场景进行了更加细致的划分。于是就有了面向服务器端的高性能计算架构DaDianNao，面向边缘端设备应用场景的ShiDianNao，面向更加泛化的机器学习算法的PuDianNao以及面向更加广泛的机器学习加速器的combricon指令集架构。</p>
<p>DaDianNao是一个多片的硬件系统，考虑到CNNs/DNNs和通用的工作负载所面临的存储墙的情况并不相同，它们虽然占用的存储空间很大，但是并没有超出多片系统的存储能力，也就是说可以考虑将数据分布在多片系统之上进行分布式的处理，可以充分的利用片上的高带宽和避免片外的低带宽，使大规模的网络模型可以更加高效的运行。</p>
<h2 id="整体硬件架构"><a href="#整体硬件架构" class="headerlink" title="整体硬件架构"></a>整体硬件架构</h2><p>DaDianNao可以看作是在DianNao基础之上的扩展，下图所示是DianNao的架构，<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/DianNao架构.png" width="400" align="center"></p>
<h3 id="单个DaDianNao结构的设计"><a href="#单个DaDianNao结构的设计" class="headerlink" title="单个DaDianNao结构的设计"></a>单个DaDianNao结构的设计</h3><p>DaDianNao的主要区别在于针对存储神经网络输入和输出数据的NBin和NBout，存储神经连接参数的SB的组织形式以及核心计算单元NFU的数据交互方式进行了针对大尺寸模型的考量，而其中最为重要的就是访存体系的设计思想和细节，DaDianDao芯片的访存体系的高层次设计图如下：<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/DaDianNao高层次结构.png" width="600" align="center"><br>左图是芯片的高层次图，每个芯片由16个tile组成，右图是单个tile的结构图。每个tile内部是一个NFU搭配四个存储SB的eDRAM bank组成，而NBin和NBout则是对应与左图eDRAM router所连接的两条灰色的额eDRAM bank。不难发现，在DaDianNao中SB是采用分布式的方式存储的，之所以采用这种方式，可能的理解如下：<br><strong>a、eDRAM虽然相对于DRAM的延迟显著减小，但是其本质仍然是DRAM，存在着漏电的效应，需要周期性的充电刷新，并且刷新的频率相对于DRAM可能会更高，这可能会对访存的性能带来一定的影响，通过拆分SB存储，可以在一定程度上减小周期性的刷新带来的影响。</strong><br><strong>b、将SB拆分，放在每个NFU周围，这样距离计算单元更近，这样在执行具体的计算的时候，访存的时延更小，性能更高。</strong></p>
<p><font color="red">DaDianNao相对于DianNao有一个比较明显的改变就是NBin/NBout/SB由原来的SDRAM存储改为eDRAM的存储介质，其只要原因如下：</font><br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/DRAM.jpg" width="400" align="center"><br>由上表可以看出，使用SRAM在一定程度上可以缩小延迟，但是其需要的面积太大，这尤其是在高性能计算芯片中面积增加程度很高，成本散热以及功耗都是很难处理的问题，选用eDRAM也是在存储密度/访存延迟/功耗之间最为适宜的权衡。</p>
<p>每个tile内部的NFU内部结构如图所示：<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/DADIANNAONFU.png" width="400" align="center"><br>图中详细描绘了NFU流水线的每个阶段与NBin/NBout/SB的交互，针对不同的网络层，根据其存储和计算的特性，其流水线的工作模式如下图所示：<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/pipeline.png" width="600" align="center"><br>其中的红色部分The eDram表示的就是SB存储，</p>
<p>&emsp;&emsp;以上是单个DaDianNao芯片的设计，由下表可知，单个DaDianNao芯片中用于NBin/NBout的central eDRAM的存储容量为4MB，每个tile中用于SB的eDRAM的容量是2MB，每个芯片由16个tile组成，所以单个芯片的eDRAM的总容量是$4+2*16=36$MB，仅仅单片的容量是无法对大模型的网络进行高性能的计算的，所以论文又提出了多片互连的架构。<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/容量.png" width="400" align="center"></p>
<h3 id="多个DaDianNao结构的互连"><a href="#多个DaDianNao结构的互连" class="headerlink" title="多个DaDianNao结构的互连"></a>多个DaDianNao结构的互连</h3><p>DaDianNao的多片互连并不是定制的设计，而是直接使用现成的技术，即HyperTransport 2.0通信IP，在每个DaDianNao芯片的上下左右四周提供四组HT 2.0的通信通道，每个通道的通信带宽在in/out方向分别达到6.6GB/s，由于相互之间数据的交换，支持全双工通信且延迟为80ns。</p>
<p>多片互连的结构可以支持大尺寸的网路模型，不同的模型在多片的模式下通信的数据量存在着较大的差异：<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/差异.png" width="600" align="center"></p>
<h2 id="相对于GPU的对比实验"><a href="#相对于GPU的对比实验" class="headerlink" title="相对于GPU的对比实验"></a>相对于GPU的对比实验</h2><p>下图是对模型训练的过程中不同的网络层在多片硬件上相对于GPU的加速的效果：<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/trainingsp.png" width="400" align="center"></p>
<p>下图是对模型前向过程中不同的网络层在多片硬件上相对于GPU的能耗比：<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/eninference.png" width="400" align="center"></p>
<p>下图是对模型训练的过程中不同的网络层在多片硬件上相对于GPU的能耗比：<br><img src="/2019/03/25/论文-DaDianNao-A-Machine-Learning-Supercomputer/entraining.png" width="400" align="center"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deepshuai.github.io/2019/03/21/论文-DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Deepshuai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/deepshuai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="努力，任何时候都不会晚。">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/21/论文-DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning/" itemprop="url">论文-DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-21T10:58:04+08:00">
                2019-03-21
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/硬件设计/" itemprop="url" rel="index">
                    <span itemprop="name">硬件设计</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;该论文是寒武纪团队的第一篇论文。论文结合了神经网络模型数据的局部性特点以及计算的特点，进行存储体系以及专用硬件的设计，从而获得更好的性能加速比以及计算功耗比。</p>
<p><strong>论文的主要贡献：</strong></p>
<font color="red">1、对大规模的CNNs和DNNs最先进机器学习算法的综合硬件设计；<br>2、在很小的芯片空间上实现高吞吐率和低功耗；<br>3、专注于优化访存的性能，着重解决”存储墙”的问题。</font>


<h2 id="神经网络的硬件化"><a href="#神经网络的硬件化" class="headerlink" title="神经网络的硬件化"></a>神经网络的硬件化</h2><p>&emsp;&emsp;神经网络的结构模拟人类大脑内部的神经元结构，每一个神经元有很多的突触，用于给其他的神经元来传递信息，所有神经元的信息累加，会使该神经元处于兴奋或者抑制状态。成千上万个神经元组合起来就是一个神经网络模型。如下图所示：<br><img src="/2019/03/21/论文-DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning/神经网络硬件化.png" width="400" align="center"><br>对于到卷积神经网络的计算，神经的突触就是就是具体的卷积运算中侧乘法步骤，而神经元就是对所有突触效果累加效果和非线性激活的处理。论文首先回顾了之前常见的全硬件实现方案(full-hardware implementation)—通常是将每个神经元都映射到具体的硬件计算单元上。这种方案虽然实现方式简洁，计算性能高，但是灵活性很差，尤其是深度学习算法快速更新，网络模型的结构以及尺寸的变化都会使该方案失效，下图是full-hardware implementationan方案的硬件关键路径的时延、芯片面积和功耗的变化趋势：<br><img src="/2019/03/21/论文-DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning/全硬件化功耗延迟和面积.png" width="400" align="center"></p>
<p>明显可以看到硬件对模型伸缩性的处理不理想的问题，为了解决这个问题，寒武纪团队提出DianNao的计算架构。</p>
<p>&emsp;&emsp;下图是DianNao内部的硬件结构，其中包含三个片上存储，分别是存储输入的NBin、存储输出的NBout以及存储神经网络模型参数的SB，三块存储均是基于SRAM实现，以获取低延时和低功耗的收益。片上存储和片外存储的数据交互方式通过DMA来完成，尽可能的节省通讯的时延。浅蓝色阴影部分是硬件逻辑模拟的神经网络结构，整个部分被称为NFU，是一个包含NFU-1、NFU-2、NFU-3三段的pipeline的结构。<br><img src="/2019/03/21/论文-DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning/DianNao加速模块.png" width="400" align="center"></p>
<p>&emsp;&emsp;<strong>NFU-1</strong>包含16个乘法单元，每个乘法单元拥有16个乘法器，因此NFU-1总共包含$16\times16=256$个乘法器，每个周期可以执行256个乘法运算； <strong>NFU-2</strong>包含16个树状的加法单元，每个加法树按照8-4-2-1个加法器的形式对数据执行加法运算；<strong>NFU-3</strong>包含16个非线性激活单元，每个激活单元对输出的数据进行非线性激活处理。对NFU逻辑结构横向观察发现，NFU的所有逻辑单元被分为16份，包含16个乘法器、$8+4+2+1=15$个加法器，一个激活单元。<br>&emsp;&emsp;整个硬件架构的核心就是三个片上存储和一个拥有三级流水的神经功能单元。根据时分复用的思想，对一个对规模的神经网络，神经网络的参数会以此加载到SB里，每层的输入数据加载到NBin中，每层的计算结果加载到NBout中，NFU提供最基础的计算逻辑(乘法、加法、非线性激活)，这种结构不会与具体的神经网络结构以及其参数规模相互绑定，相较与之前full-hardware implementation的硬件加速器设计，在结构和模型尺寸的灵活性上有很大的改进。</p>
<h2 id="优化细节"><a href="#优化细节" class="headerlink" title="优化细节"></a>优化细节</h2><h3 id="16位定点数代替32位浮点数"><a href="#16位定点数代替32位浮点数" class="headerlink" title="16位定点数代替32位浮点数"></a>16位定点数代替32位浮点数</h3><p>&emsp;&emsp;16位定点数代替32位浮点数的前提：神经网路模型的精度不能显著降低。在模型压缩和加速算法研究领域已经出现很多的研究成果，利用8-bit定点数和16-bit定点数来代替原模型32位浮点数据，并且模型的精度基本没有损失，所以，DianNao论文采用低精度的策略也是建立在算法研究的基础之上的，如下图：<br><img src="/2019/03/21/论文-DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning/mnist精度.png" width="400" align="center"><br>mnist数据集(一个较小的手写数字分类的数据集)上的神经网络在16-bit定点数和32位浮点数时对应的分类错误率，相差很小，基本可以忽略。</p>
<p>而下图是两种精度下芯片的面积和功耗比较：<br><img src="/2019/03/21/论文-DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning/面积和功耗.png" width="400" align="center"><br>在很少的精度损失的前提下，芯片的面积将为原来的1/6，而功耗也仅仅为1/7不到，这一种很好的权衡策略。</p>
<h3 id="NBin-NBout-SB存储模块分离"><a href="#NBin-NBout-SB存储模块分离" class="headerlink" title="NBin/NBout/SB存储模块分离"></a>NBin/NBout/SB存储模块分离</h3><p>虽然三个存储模块都是片上的SRAM，但是三个模块是彼此独立的，其主要原因是不同的访存带宽，NBin/NBout的访存单位是向量，SB的访存单位是矩阵，不同的访存宽度在功耗上有很明显的差异：<br><img src="/2019/03/21/论文-DianNao-A-Small-Footprint-High-Throughput-Accelerator-for-Ubiquitous-Machine-Learning/访存宽度和功耗.png" width="400" align="center"><br>拆分成不同的模块，可以在性能和功耗之间找到很好的平衡点，可以理解层对内存带宽采用更加细粒度的分析。而将NBin和NBout也进行拆分主要是为了降低数据相关性，NBin和NBout类似于cache用来缓存模型计算过程中数据的输入和输出，两种数据访问的模式可能不同，将其放入不同的SRAM中，减少了数据相关性，在一定程度上提升了性能，简化逻辑的同时降低了功耗。</p>
<h3 id="考虑NBin和SB数据局部性"><a href="#考虑NBin和SB数据局部性" class="headerlink" title="考虑NBin和SB数据局部性"></a>考虑NBin和SB数据局部性</h3><p>该思想与流水线设计思想一致，就是将数据的加载和计算过程进行overlap，当对当前数据进行计算的同时，通过DMA对下一组数据的加载，该功能的实现需要比较精细的逻辑同步，同时要求NBin/SB满足双端口的访问，这在一定程度上会增加功耗。</p>
<h3 id="考虑NBout数据局部性"><a href="#考虑NBout数据局部性" class="headerlink" title="考虑NBout数据局部性"></a>考虑NBout数据局部性</h3><p>在NFU中增加专用的寄存器，如果计算数据的规模比较大，一次NBin只能加载部分的数据，那么计算的结果也就是最终结果的一部分，这时增加的寄存器暂存partial的结果，用于快速的结果组合，减少计算结果片外写入和片内读入的开销，在一定程度上有性能的提升。</p>
<h2 id="实验及结果分析"><a href="#实验及结果分析" class="headerlink" title="实验及结果分析"></a>实验及结果分析</h2><p>对多种尺寸的卷积层、全连接层和池化层进行实验分析，实验的baseline是CPU的ISMD计算，该设计在芯片面积，功耗以及计算时延方面都有数量级的提升，具体的结果参考论文<font color="ff00"><a href="http://boogie.is.titech.ac.jp/lecture/lecture-wiki/index.php?plugin=attach&amp;refer=hpc2015&amp;openfile=DianNao.pdf" target="_blank" rel="noopener">DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous</a></font></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deepshuai.github.io/2019/03/18/中科院计算所寒武纪团队DianNao系列论文导读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Deepshuai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/deepshuai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="努力，任何时候都不会晚。">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/18/中科院计算所寒武纪团队DianNao系列论文导读/" itemprop="url">中科院计算所寒武纪团队DianNao系列论文导读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-18T15:57:58+08:00">
                2019-03-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/硬件设计/" itemprop="url" rel="index">
                    <span itemprop="name">硬件设计</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;中科院计算所的这几篇DianNao系列论文引发了专用深度学习加速器研究和应用的热潮，后续的博客将会读每一篇论文进行阅读和分析，在每一篇的论文阅读之前，我们先梳理一下寒武纪芯片DianNao的整体项目。</p>
<h2 id="DianNao项目之前："><a href="#DianNao项目之前：" class="headerlink" title="DianNao项目之前："></a>DianNao项目之前：</h2><p>&emsp;&emsp;2010年，Temam教授在ISCA的主题报告上提到，机器学习硬件加速器是处理器微结构领域极有吸引力的一个发展方向，是处理器技术、应用和机器学习发展的大势所趋。在2012年的ISCA上，Temam教授提出了第一个机器学习加速器设计，表明在以神经网络为基础的一大类应用上是可以以很小的面积和功耗获得高性能的。但此工作的主要局限性在于其内存带宽。</p>
<h2 id="DianNao项目："><a href="#DianNao项目：" class="headerlink" title="DianNao项目："></a>DianNao项目：</h2><p>&emsp;&emsp;DianNao学术项目的目标是面向机器学习研究加速器架构。本项目是中科院计算所的陈云霁教授和法国Inria的Olivier Temam间的一个学术合作项目，双方为此设立了联合实验室。</p>
<p>&emsp;&emsp;Temam教授和陈教授的合作始于第一个加速器，名为<font color="ff00"><a href="http://boogie.is.titech.ac.jp/lecture/lecture-wiki/index.php?plugin=attach&amp;refer=hpc2015&amp;openfile=DianNao.pdf" target="_blank" rel="noopener">DianNao</a></font>（这也是DianNao家族的第一个成员）。DianNao在ISCA-2012加速器的基础上增加了局部存储，使其可以捕捉深度神经网路的数据局部性并由此克服内存带宽的限制。DianNao加速器的设计发表于ASPLOS-2014，获得了该会议的最佳论文奖。</p>
<p>&emsp;&emsp;DianNao家族的第二个加速器是DianNao的多片版本，有两个主要的设计目标：一是揭示神经网络层的可分特性使得加速器可具备极好的可扩展性，二是聚集足够多的片上存储来将整个机器学习模型都放在片上，从而克服内存带宽的限制。这个被称为<font color="ff00"><a href="http://novel.ict.ac.cn/ychen/pdf/DaDianNao.pdf" target="_blank" rel="noopener">DaDianNao</a></font>的设计发表在MICRO-2014上，获得了该会议的最佳论文奖。</p>
<p>&emsp;&emsp;作为克服嵌入式应用中内存带宽限制的另一种方法，我们揭示可以通过加速器和传感器的直连来绕过内存。我们将此思想应用于视觉传感器，从而提出了DianNao家族的第三个加速器<font color="ff00"><a href="https://dl.acm.org/citation.cfm?doid=2749469.2750389" target="_blank" rel="noopener">ShiDianNao</a></font>，发表于2015年的ISCA上。</p>
<p>&emsp;&emsp;最后，我们也揭示这类加速器的应用领域可以被拓展至多种机器学习算法，因为这些算法多具有类似的运算操作。相应的加速器设计称为<font color="#ff00"><a href="https://dl.acm.org/citation.cfm?doid=2694344.2694358" target="_blank" rel="noopener">PuDianNao</a></font>（DianNao家族的第四个以及最后一个成员），发表于ASPLOS-2015。</p>
<h2 id="DianNao项目之后"><a href="#DianNao项目之后" class="headerlink" title="DianNao项目之后"></a>DianNao项目之后</h2><p>&emsp;&emsp;陈云霁教授和他的中科院计算所团队为一大类神经网络加速器设计了一套名为<font color="ff00"><a href="https://dl.acm.org/citation.cfm?id=3001179" target="_blank" rel="noopener">Cambricon的指令集</a></font>。该指令集发表于ISCA-2016，在该会议的同行评议中获得了最高分。</p>
<p>&emsp;&emsp;同时针对深度学习模型中的稀疏化特点，又提出了一种对稀疏权重的矩阵运算加速的架构<a href="https://ieeexplore.ieee.org/document/7783723" target="_blank" rel="noopener">Cambricon-X</a>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deepshuai.github.io/2019/01/01/写在最前面/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Deepshuai">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/deepshuai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="努力，任何时候都不会晚。">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/01/写在最前面/" itemprop="url">写在最前面</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-01T15:57:58+08:00">
                2019-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/个人感悟/" itemprop="url" rel="index">
                    <span itemprop="name">个人感悟</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>&emsp;&emsp;之前尝试着些CSND博客，写了不少东西很杂很乱。“本科教会你如何学习，研究生教会你如何工作。”这句话一直认为颇有道理，本渣(绝对不是一种谦虚的心态，是真的渣)本科阶段懵懵懂懂没有规划，硕士阶段恍恍惚惚没有刻苦沉淀就这么过来了，可能最大的收获就是找到了工作和学习的方向，人生不经该给自己设限，但设限让自己更加专注，也更能深入的积累相关的技术，这也是开这个博客的原因，方向有了，剩下的就看自己的努力了。</p>
<p>&emsp;&emsp;说到方向，也算是被洪流裹挟着的一员(深度学习太火)，硕士阶段尝试的方向不少，经过写论文(深度学习算法优化)、实习(智能硬件)和工作(深度学习&amp;智能硬件)才渐渐变得清晰明了。所以这个博客会重点关注深度学习和AI芯片的最新进展。写东西是慢慢优化的过程，刚开始写自己有时候都看着不太清晰明了，多多交流。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/avatar/deepshuai.jpg" alt="Deepshuai">
            
              <p class="site-author-name" itemprop="name">Deepshuai</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Deepshuai</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
